\begin{table}[htbp]
\centering
\caption{Overview of Image Classification Datasets}
\label{tab:datasets}
\begin{tabular}{llrrr}
\toprule
 & Task & Size & Num Classes & Average Image Size \\
\midrule
\href{https://www.cs.toronto.edu/~kriz/cifar.html}{\texttt{CIFAR10}} & General objects & 60,000 & 10 & 32x32 \\
\href{https://www.cs.toronto.edu/~kriz/cifar.html}{\texttt{CIFAR100}} & General objects & 60,000 & 100 & 32x32 \\
\href{https://data.caltech.edu/records/mzrjq-6wc02}{\texttt{Caltech101}} & General objects & 9,146 & 101 & 300x200 \\
\href{https://data.caltech.edu/records/20087}{\texttt{Caltech256}} & General objects & 30,607 & 256 & 300x200 \\
\href{https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html}{\texttt{CelebA}} & Face & 202,599 & 10177 & 178x218 \\
\href{https://github.com/openai/CLIP/blob/main/data/country211.md}{\texttt{Country211}} & Countries & 75,000 & 249 & 468x382 \\
\href{https://www.robots.ox.ac.uk/~vgg/data/dtd/}{\texttt{DTD}} & Textures & 5,640 & 47 & 494x450 \\
\href{https://www.westernsydney.edu.au/icns/resources/reproducible_research3/publication_support_materials2/emnist}{\texttt{EMNIST}} & Handwriting & 814,255 & 47 & 28x28 \\
\href{https://github.com/phelber/eurosat}{\texttt{EuroSAT}} & Land & 27,000 & 10 & 64x64 \\
\href{https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge}{\texttt{FER2013}} & Faces & 35,887 & 7 & 48x48 \\
\href{https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/}{\texttt{FGVCAircraft}} & Aircraft & 10,000 & 100 & 1101x749 \\
\href{https://github.com/zalandoresearch/fashion-mnist}{\texttt{FashionMNIST}} & Clothing & 70,000 & 10 & 28x28 \\
\href{https://www.robots.ox.ac.uk/~vgg/data/flowers/102/}{\texttt{Flowers102}} & Flowers & 7,169 & 102 & 631x534 \\
\href{https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/}{\texttt{Food101}} & Food & 101,000 & 101 & 512x512 \\
\href{https://benchmark.ini.rub.de/}{\texttt{GTSRB}} & Traffic Signs & 51,840 & 43 & 50x50 \\
\href{https://github.com/fastai/imagenette#imagenette-1}{\texttt{Imagenette}} & General objects & 13,394 & 10 & 471x408 \\
\href{https://github.com/rois-codh/kmnist}{\texttt{KMNIST}} & Handwriting & 70,000 & 10 & 28x28 \\
\href{http://vis-www.cs.umass.edu/lfw/}{\texttt{LFWPeople}} & Faces & 13,233 & 5749 & 250x250 \\
\href{http://yann.lecun.com/exdb/mnist/}{\texttt{MNIST}} & Handwriting & 70,000 & 10 & 28x28 \\
\href{https://github.com/brendenlake/omniglot}{\texttt{Omniglot}} & Handwriting & 32,460 & 964 & 105x105 \\
\href{https://www.robots.ox.ac.uk/~vgg/data/pets/}{\texttt{OxfordIIITPet}} & Animals & 7,400 & 37 & 437x391 \\
\href{https://github.com/basveeling/pcam}{\texttt{PCAM}} & Medical scans & 327,680 & 2 & 96x96 \\
\href{https://github.com/facebookresearch/qmnist}{\texttt{QMNIST}} & Handwriting & 120,000 & 10 & 28x28 \\
\href{https://github.com/openai/CLIP/blob/main/data/rendered-sst2.md}{\texttt{RenderedSST2}} & Sentiment from text images & 8,741 & 2 & 448x448 \\
\href{https://archive.ics.uci.edu/dataset/178/semeion+handwritten+digit}{\texttt{SEMEION}} & Handwriting & 1,593 & 10 & 16x16 \\
\href{https://cs.stanford.edu/~acoates/stl10/}{\texttt{STL10}} & General objects & 13,000 & 10 & 96x96 \\
\href{https://vision.princeton.edu/projects/2010/SUN/}{\texttt{SUN397}} & Scenes & 108,753 & 397 & 959x775 \\
\href{http://ufldl.stanford.edu/housenumbers/}{\texttt{SVHN}} & Digits & 630,420 & 10 & 32x32 \\
\href{https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset}{\texttt{StanfordCars}} & Cars & 16,185 & 196 & 700x483 \\
\href{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps}{\texttt{USPS}} & Handwriting & 9,298 & 10 & 16x16 \\
\bottomrule
\end{tabular}
\end{table}
